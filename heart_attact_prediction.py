# -*- coding: utf-8 -*-
"""Heart- attact Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W0Yy7B7EZxoWjBtqo7fzoih7fK7fz47j
"""

#importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/heart.csv')
df.head()

print(df.shape)
print(df.info())
df.describe()

df.isnull().sum()

"""# Data Exploration
Now we can plot the distribution of data wrt dependent variable i.e HeartDisease
"""

sns.pairplot(df,hue='HeartDisease')

sns.set_theme(style="whitegrid")
sns.boxplot(x="Age", data=df, palette="Set3")
plt.title("Age Distribution")

fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax)

df.HeartDisease.value_counts().plot(kind='bar')
plt.xlabel("Heart Diseases or Not")
plt.ylabel("Count")
plt.title("Heart Diseases")

"""## Data Preprocessing"""

cat = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']

df[cat]

from sklearn.preprocessing import LabelEncoder
lb=LabelEncoder()
df[cat] = df[cat].apply(lb.fit_transform)

df[cat]

X=df.drop('HeartDisease',axis=1)
X.head()

y=df['HeartDisease']
y.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)

X_train.shape

from sklearn.preprocessing import QuantileTransformer
scaler=QuantileTransformer()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)


# Transform features using quantiles information.

# This method transforms the features to follow a uniform or a normal distribution.
# Therefore, for a given feature, this transformation tends to spread out the most frequent values.
# It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.

# The transformation is applied on each feature independently. First an estimate of the cumulative distribution
# function of a feature is used to map the original values to a uniform distribution. The obtained values are then
# mapped to the desired output distribution using the associated quantile function. Features values of new/unseen
# data that fall below or above the fitted range will be mapped to the bounds of the output distribution.
# Note that this transform is non-linear. It may distort linear correlations between variables measured at the
# same scale but renders variables measured at different scales more directly comparable

X_train[0]

"""# Using KNN
K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems. However, it is mainly used for classification predictive problems in industry. The following two properties would define KNN well −

Lazy learning algorithm − KNN is a lazy learning algorithm because it does not have a specialized training phase and uses all the data for training while classification.

Non-parametric learning algorithm − KNN is also a non-parametric learning algorithm because it doesn’t assume anything about the underlying data.
"""

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier(n_neighbors=4,metric='euclidean',p=1)
knn.fit(X_train,y_train)

y_pred=knn.predict(X_test)
y_pred

knn.score(X_test,y_test)

from sklearn.metrics import accuracy_score
from sklearn import metrics

metrics.accuracy_score(y_test,y_pred)

from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y_test, y_pred)
mat

from sklearn.metrics import classification_report
target_names = ['Heart Diseases', 'Normal']
print(classification_report(y_test, y_pred, target_names=target_names))

"""To select optimize k value we will use elbow method"""

#For selecting K value
error_rate = []

# Will take some time
for i in range(1,40):

    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))

import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='red', linestyle='dashed', marker='o',
         markerfacecolor='green', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

#From graph we can see that optimize k value is 16,17,18
# Now we will train our KNN classifier with this k values

knn=KNeighborsClassifier(n_neighbors=3,metric='euclidean',p=2)
knn.fit(X_train,y_train)

knn.score(X_test,y_test)

from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(mat, annot=True)

from sklearn.metrics import classification_report
target_names = ['Diabetes', 'Normal']
print(classification_report(y_test, y_pred, target_names=target_names))

"""## Using H2o.ai AutoML"""

!pip install h2o

import h2o
# We will be using default parameter Here with H2O init method
h2o.init()

"""Covert to H2O Data frame"""

hf = h2o.H2OFrame(df)

# Data Transform - Split train : test datasets
train, valid = hf.split_frame(ratios = [.80], seed = 1234)
print("Training Dataset", train.shape)
print("Validation Dataset", valid.shape)

train.head(5)

valid.head()

# Identify predictors and response
featureColumns = train.columns
targetColumn   = "HeartDisease"
featureColumns.remove(targetColumn)

"""###AutoML does all of this automatically:

✅ Data preprocessing (handles missing values, encoding, etc.)

✅ Tries many ML algorithms (like:

Gradient Boosting Machine (GBM)

XGBoost

Random Forest (DRF)

Generalized Linear Model (GLM)

Deep Learning (Neural Nets)

And combines them using Stacked Ensemble)

✅ Tunes hyperparameters (e.g., number of trees, learning rate)

✅ Evaluates each model using the validation_frame you provided

✅ Ranks all models in a leaderboard, from best to worst

✅ Selects the best model for predictions
"""

import time
from h2o.automl import H2OAutoML


aml = H2OAutoML(max_models=12,         # Train up to 12 different models
                seed=1234,             # Set random seed for reproducibility
                balance_classes=True   # Automatically balance classes if dataset is imbalanced
               )


aml.train(x=featureColumns,            # List of input feature column names
          y=targetColumn,              # Target/output column to predict (e.g., 'Class')
          training_frame=train,        # Training dataset (used to train models)
          validation_frame=valid)      # Validation dataset (used to evaluate model performance during training)

"""### Get the full leaderboard of all models trained by AutoML"""

lb = aml.leaderboard  # 'lb' now holds a ranked list of all models based on performance

# Print the entire leaderboard (all rows)
print(lb.head(rows = lb.nrows))  # Shows all models with their metrics like AUC, RMSE, logloss, etc.

"""#### Generate detailed model explanation using validation data"""

exa = aml.explain(valid)  # Creates visual reports like feature importance, SHAP values, ROC, etc. for the best model

# Evaluate the best model with testing data.
model = aml.leader

!pip install scikit-plot

!pip install scipy==1.9.3

# For Classification
import scikitplot as skplt
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import cohen_kappa_score, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

# Predict with the best model.
predicted_y = model.predict(valid[featureColumns])

predicted_data = predicted_y.as_data_frame()
valid_dataset = valid.as_data_frame()

# Evaluate the skill of the Trained model
acc                 = accuracy_score(valid_dataset[targetColumn], np.round(abs(predicted_data['predict'])))
classReport         = classification_report(valid_dataset[targetColumn], np.round(abs(predicted_data['predict'])))
confMatrix          = confusion_matrix(valid_dataset[targetColumn], np.round(abs(predicted_data['predict'])))

print(); print('Testing Results of the trained model: ')
print(); print('Accuracy : ', acc)
print(); print('Confusion Matrix :\n', confMatrix)
print(); print('Classification Report :\n',classReport)
# Confusion matrix
skplt.metrics.plot_confusion_matrix(valid_dataset[targetColumn], np.round(abs(predicted_data['predict'])), figsize=(7,7)); plt.show()

